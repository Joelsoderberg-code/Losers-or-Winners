"""HÃ¤mtar dagliga OHLCV-data frÃ¥n Polygon API och skriver till CSV.

Funktionalitet:
- LÃ¤ser API-nyckel och parametrar frÃ¥n argument eller miljÃ¶variabler/.env
- BestÃ¤mmer datumintervall: START/END, BACKFILL_DAYS eller default "igÃ¥r"
- Skriver CSV med kolumner: ticker, timestamp, open, close, volume

AnvÃ¤ndning (via DAG eller direkt):
    fetch_data_from_api()
"""

# Importerar alla nÃ¶dvÃ¤ndiga bibliotek:
import csv
import os
from datetime import datetime, timedelta

from dotenv import load_dotenv
from polygon import RESTClient


def fetch_data_from_api(
    api_key: str | None = None,
    ticker: str | None = None,
    output_path: str | None = None,
) -> None:
    """HÃ¤mta aggregerade dagsdata och skriv till CSV.

    Parametrar prioriteras i ordning: funktionsargument â†’ miljÃ¶variabler/.env â†’ defaults.
    """
    # LÃ¤s in .env och miljÃ¶variabler om argument inte ges
    load_dotenv()
    api_key = api_key or os.getenv("POLYGON_API_KEY")
    if not api_key:
        raise ValueError("POLYGON_API_KEY saknas i .env/miljÃ¶n och gavs inte som argument")

    ticker = ticker or os.getenv("TICKER", "SPY")

    if output_path is None:
        # Lokalt defaultar vi till projektmappen; i Composer skrivs detta
        # Ã¶ver via Airflow Variables till /home/airflow/gcs/data.
        output_dir = os.getenv("OUTPUT_DIR", "/home/joel/Losers-or-Winners/data")
        output_file = os.getenv("OUTPUT_FILE", "stock_data.csv")
        output_path = os.path.join(output_dir, output_file)

    # BestÃ¤m intervall:
    # 1) START_DATE/END_DATE om bÃ¥da finns
    # 2) BACKFILL_DAYS (t.ex. 365) â†’ frÃ¥n (igÃ¥r - BACKFILL_DAYS + 1) till igÃ¥r
    # 3) annars: endast igÃ¥r
    today = datetime.now().date()
    yesterday = today - timedelta(days=1)

    start_env = os.getenv("START_DATE")
    end_env = os.getenv("END_DATE")
    backfill_days_str = os.getenv("BACKFILL_DAYS", "0")
    try:
        backfill_days = int(backfill_days_str)
    except ValueError:
        backfill_days = 0

    if start_env and end_env:
        start_date = start_env
        end_date = end_env
    elif backfill_days > 0:
        start_date = (yesterday - timedelta(days=backfill_days - 1)).isoformat()
        end_date = yesterday.isoformat()
    else:
        start_date = end_date = yesterday.isoformat()

    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    client = RESTClient(api_key)
    # FÃ¶rsÃ¶k hÃ¤mta fler rader och i stigande ordning
    aggs = client.get_aggs(ticker, 1, "day", start_date, end_date, limit=50000, sort="asc")

    if not aggs:
        print("âš ï¸ Ingen data returnerades frÃ¥n Polygon.io")
        return

    print(f"ğŸ“Š HÃ¤mtade {len(aggs)} datapunkter fÃ¶r {ticker} ({start_date}..{end_date})")

    rows = []
    for bar in aggs:
        rows.append(
            {
                "ticker": ticker,
                "timestamp": datetime.fromtimestamp(bar.timestamp / 1000).isoformat(),
                "open": bar.open,
                "close": bar.close,
                "volume": int(bar.volume) if bar.volume is not None else 0,
            }
        )

    with open(output_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=["ticker", "timestamp", "open", "close", "volume"])
        writer.writeheader()
        writer.writerows(rows)

    print(f"âœ… Skrev {len(rows)} rader till {output_path}")
